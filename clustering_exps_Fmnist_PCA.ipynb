{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clustering-exps-Fmnist-PCA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMVxeD4vXAZB",
        "cellView": "form",
        "outputId": "1e9dbce6-834c-41e7-b835-85a5026ae3e1"
      },
      "source": [
        "#@title Don't forget to upload usps.h5\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "def purity_score(y_true, y_pred): # from https://stackoverflow.com/a/51672699/7947996; in [0,1]; 0-bad,1-good\n",
        "    # compute contingency matrix (also called confusion matrix)\n",
        "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
        "    # return purity\n",
        "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
        "\n",
        "from sklearn.metrics.cluster import adjusted_rand_score # in [0,1]; 0-bad,1-good\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score # in [0,1]; 0-bad,1-good\n",
        "\n",
        "!pip install coclust\n",
        "from coclust.evaluation.external import accuracy # in [0,1]; 0-bad,1-good\n",
        "\n",
        "def get_data_20news():\n",
        "  import tensorflow as tf\n",
        "  from sklearn.datasets import fetch_20newsgroups\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "  _20news = fetch_20newsgroups(subset=\"all\")\n",
        "  data = _20news.data\n",
        "  target = _20news.target\n",
        "\n",
        "  vectorizer = TfidfVectorizer(max_features=2000)\n",
        "  data = vectorizer.fit_transform(data)\n",
        "  data = data.toarray()\n",
        "\n",
        "  return data, target\n",
        "\n",
        "\n",
        "def get_data_mnist():\n",
        "  import tensorflow as tf\n",
        "  mnist = tf.keras.datasets.mnist\n",
        "  (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "  x_train = np.concatenate((x_train,x_test))\n",
        "  y_train = np.concatenate((y_train,y_test))\n",
        "\n",
        "  real_labels = y_train\n",
        "\n",
        "  # # indices = np.isin(y_train,range(number_of_dist))\n",
        "  # x_train = x_train[indices]\n",
        "  # y_train = y_train[indices]\n",
        "\n",
        "  samples = (x_train.reshape((x_train.shape[0],-1))/255.).astype(np.float32)\n",
        "  \n",
        "  return samples, real_labels\n",
        "\n",
        "def get_data_fmnist():\n",
        "  import tensorflow as tf\n",
        "  mnist = tf.keras.datasets.fashion_mnist\n",
        "  (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "  x_train = np.concatenate((x_train,x_test))\n",
        "  y_train = np.concatenate((y_train,y_test))\n",
        "\n",
        "  real_labels = y_train\n",
        "\n",
        "  # # indices = np.isin(y_train,range(number_of_dist))\n",
        "  # x_train = x_train[indices]\n",
        "  # y_train = y_train[indices]\n",
        "\n",
        "  samples = (x_train.reshape((x_train.shape[0],-1))/255.).astype(np.float32)\n",
        "  \n",
        "  return samples, real_labels\n",
        "\n",
        "def get_data_usps():\n",
        "  import h5py\n",
        "  path = \"./usps.h5\"\n",
        "  with h5py.File(path, 'r') as hf:\n",
        "    train = hf.get('train')\n",
        "    X_tr = train.get('data')[:]\n",
        "    y_tr = train.get('target')[:]\n",
        "    test = hf.get('test')\n",
        "    X_te = test.get('data')[:]\n",
        "    y_te = test.get('target')[:]\n",
        "\n",
        "  samples = np.concatenate((X_tr,X_te))\n",
        "  real_labels = np.concatenate((y_tr,y_te))\n",
        "  return samples, real_labels\n",
        "\n",
        "original_data_name = \"fmnist\" # @param [\"mnist\", \"fmnist\", \"20news\", \"usps\"]\n",
        "\n",
        "if original_data_name == \"mnist\":\n",
        "    samples, real_labels = get_data_mnist()\n",
        "elif original_data_name == \"fmnist\":\n",
        "    samples, real_labels = get_data_fmnist()\n",
        "elif original_data_name == \"20news\":\n",
        "    samples, real_labels = get_data_20news()\n",
        "elif original_data_name == \"usps\":\n",
        "    samples, real_labels = get_data_usps()\n",
        "  \n",
        "k = len(np.unique(real_labels))\n",
        "n_init = 10\n",
        "dim_pca = 100\n",
        "\n",
        "if dim_pca is not None:\n",
        "    import numpy as np\n",
        "    from sklearn.decomposition import PCA\n",
        "    X = samples\n",
        "    pca = PCA(n_components=dim_pca)\n",
        "    samples = pca.fit_transform(X)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting coclust\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/44/ad5a69c7187c2b7bcf2c45596e9052811a3be52f4fcaa6709937c5146ee2/coclust-0.2.1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from coclust) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from coclust) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from coclust) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->coclust) (1.0.1)\n",
            "Building wheels for collected packages: coclust\n",
            "  Building wheel for coclust (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for coclust: filename=coclust-0.2.1-cp37-none-any.whl size=29871 sha256=97037498ed9a16ef4c9974f337a9f4dae33fab4391569c4eddae04efaba91126\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/d7/68/df601d0b5f8b934cf890dc626c2271df381fb0c3e910b0a34e\n",
            "Successfully built coclust\n",
            "Installing collected packages: coclust\n",
            "Successfully installed coclust-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM2n39n8x0fE"
      },
      "source": [
        "### Random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExRzemEgvWeb",
        "outputId": "8923a3c9-8928-4cf6-d6c7-6c8aa2b984ab"
      },
      "source": [
        "predicted_random = np.random.randint(k,size=len(real_labels))\n",
        "\n",
        "print(purity_score(real_labels,predicted_random))\n",
        "print(adjusted_rand_score(real_labels,predicted_random))\n",
        "print(normalized_mutual_info_score(real_labels,predicted_random))\n",
        "print(accuracy(real_labels,predicted_random))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10544285714285714\n",
            "7.021638035521908e-06\n",
            "0.0002652432490291231\n",
            "0.10452857142857143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE1OLiiGx3Tl"
      },
      "source": [
        "### k-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fSvnYVhb930",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764e6640-ae84-42fd-e57a-3346fcc4214d"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "X = samples\n",
        "kmeans = KMeans(n_clusters=k,n_init=n_init).fit(X)\n",
        "predicted_km = kmeans.predict(X)\n",
        "\n",
        "print(purity_score(real_labels,predicted_km))\n",
        "print(adjusted_rand_score(real_labels,predicted_km))\n",
        "print(normalized_mutual_info_score(real_labels,predicted_km))\n",
        "print(accuracy(real_labels,predicted_km))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5757571428571429\n",
            "0.3741965909771554\n",
            "0.5124030132172156\n",
            "0.5399714285714285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9XY7cupOKay",
        "outputId": "da48c6c0-d6f5-4321-d51a-1c74c851d600"
      },
      "source": [
        "import sklearn.metrics\n",
        "matrix = sklearn.metrics.cluster.contingency_matrix(real_labels, predicted_km)\n",
        "print(matrix)\n",
        "print(matrix/matrix.sum(axis=1, keepdims=True))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 198   23   34   47 1984    0  499    3  208 4004]\n",
            " [6296    2    0   55  186    0  145    0   30  286]\n",
            " [  12   24   27 2162 2019    0  469    1 2188   98]\n",
            " [3634    5   11   66  798    1  477    0   20 1988]\n",
            " [ 172   15   30 3546 1072    0  228    0 1122  815]\n",
            " [   1   13   12    0   59  562 4695 1655    0    3]\n",
            " [  61   56   19 1392 2426    2  726    6 1089 1223]\n",
            " [   0    7    0    0    1  338  733 5921    0    0]\n",
            " [  24 2510 2836   43  385    8  491  332  347   24]\n",
            " [   3    7    5   12   69 5881  236  778    2    7]]\n",
            "[[2.82857143e-02 3.28571429e-03 4.85714286e-03 6.71428571e-03\n",
            "  2.83428571e-01 0.00000000e+00 7.12857143e-02 4.28571429e-04\n",
            "  2.97142857e-02 5.72000000e-01]\n",
            " [8.99428571e-01 2.85714286e-04 0.00000000e+00 7.85714286e-03\n",
            "  2.65714286e-02 0.00000000e+00 2.07142857e-02 0.00000000e+00\n",
            "  4.28571429e-03 4.08571429e-02]\n",
            " [1.71428571e-03 3.42857143e-03 3.85714286e-03 3.08857143e-01\n",
            "  2.88428571e-01 0.00000000e+00 6.70000000e-02 1.42857143e-04\n",
            "  3.12571429e-01 1.40000000e-02]\n",
            " [5.19142857e-01 7.14285714e-04 1.57142857e-03 9.42857143e-03\n",
            "  1.14000000e-01 1.42857143e-04 6.81428571e-02 0.00000000e+00\n",
            "  2.85714286e-03 2.84000000e-01]\n",
            " [2.45714286e-02 2.14285714e-03 4.28571429e-03 5.06571429e-01\n",
            "  1.53142857e-01 0.00000000e+00 3.25714286e-02 0.00000000e+00\n",
            "  1.60285714e-01 1.16428571e-01]\n",
            " [1.42857143e-04 1.85714286e-03 1.71428571e-03 0.00000000e+00\n",
            "  8.42857143e-03 8.02857143e-02 6.70714286e-01 2.36428571e-01\n",
            "  0.00000000e+00 4.28571429e-04]\n",
            " [8.71428571e-03 8.00000000e-03 2.71428571e-03 1.98857143e-01\n",
            "  3.46571429e-01 2.85714286e-04 1.03714286e-01 8.57142857e-04\n",
            "  1.55571429e-01 1.74714286e-01]\n",
            " [0.00000000e+00 1.00000000e-03 0.00000000e+00 0.00000000e+00\n",
            "  1.42857143e-04 4.82857143e-02 1.04714286e-01 8.45857143e-01\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [3.42857143e-03 3.58571429e-01 4.05142857e-01 6.14285714e-03\n",
            "  5.50000000e-02 1.14285714e-03 7.01428571e-02 4.74285714e-02\n",
            "  4.95714286e-02 3.42857143e-03]\n",
            " [4.28571429e-04 1.00000000e-03 7.14285714e-04 1.71428571e-03\n",
            "  9.85714286e-03 8.40142857e-01 3.37142857e-02 1.11142857e-01\n",
            "  2.85714286e-04 1.00000000e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57lT0F8Mx4Fh"
      },
      "source": [
        "### GMM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxjlKBYscsE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828eaa65-dcf5-4b56-e27a-51f2fc100101"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "X = samples\n",
        "gm = GaussianMixture(n_components=k,n_init=n_init).fit(X)\n",
        "predicted_gmm = gm.predict(X)\n",
        "\n",
        "print(purity_score(real_labels,predicted_gmm))\n",
        "print(adjusted_rand_score(real_labels,predicted_gmm))\n",
        "print(normalized_mutual_info_score(real_labels,predicted_gmm))\n",
        "print(accuracy(real_labels,predicted_gmm))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5529571428571428\n",
            "0.37112165905915\n",
            "0.5726910419313281\n",
            "0.5027\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOjm81-zyy3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33bbe191-3540-47ac-b1b2-0589ebbc475d"
      },
      "source": [
        "import sklearn.metrics\n",
        "matrix = sklearn.metrics.cluster.contingency_matrix(real_labels, predicted_gmm)\n",
        "print(matrix)\n",
        "print(matrix/matrix.sum(axis=1, keepdims=True))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   3   86   36    0    0  610 2014    0 4232   19]\n",
            " [   0    1 6117    0    0  650  231    0    1    0]\n",
            " [   0 4539    1    0    0   86 2343    0   23    8]\n",
            " [   0  119 1651    0    0 4149  861    0  219    1]\n",
            " [   0 4565    2    0    0  928 1493    2    6    4]\n",
            " [1843    0    0 3085   31    6   25    0    0 2010]\n",
            " [   0 3016   11    0    0  361 2529    0 1064   19]\n",
            " [  46    0    0 4941 1382    0    0    0    0  631]\n",
            " [   2    3    3    7    0  253 1225 3020    0 2487]\n",
            " [3041    0    0   72 3626    0   14    0    0  247]]\n",
            "[[4.28571429e-04 1.22857143e-02 5.14285714e-03 0.00000000e+00\n",
            "  0.00000000e+00 8.71428571e-02 2.87714286e-01 0.00000000e+00\n",
            "  6.04571429e-01 2.71428571e-03]\n",
            " [0.00000000e+00 1.42857143e-04 8.73857143e-01 0.00000000e+00\n",
            "  0.00000000e+00 9.28571429e-02 3.30000000e-02 0.00000000e+00\n",
            "  1.42857143e-04 0.00000000e+00]\n",
            " [0.00000000e+00 6.48428571e-01 1.42857143e-04 0.00000000e+00\n",
            "  0.00000000e+00 1.22857143e-02 3.34714286e-01 0.00000000e+00\n",
            "  3.28571429e-03 1.14285714e-03]\n",
            " [0.00000000e+00 1.70000000e-02 2.35857143e-01 0.00000000e+00\n",
            "  0.00000000e+00 5.92714286e-01 1.23000000e-01 0.00000000e+00\n",
            "  3.12857143e-02 1.42857143e-04]\n",
            " [0.00000000e+00 6.52142857e-01 2.85714286e-04 0.00000000e+00\n",
            "  0.00000000e+00 1.32571429e-01 2.13285714e-01 2.85714286e-04\n",
            "  8.57142857e-04 5.71428571e-04]\n",
            " [2.63285714e-01 0.00000000e+00 0.00000000e+00 4.40714286e-01\n",
            "  4.42857143e-03 8.57142857e-04 3.57142857e-03 0.00000000e+00\n",
            "  0.00000000e+00 2.87142857e-01]\n",
            " [0.00000000e+00 4.30857143e-01 1.57142857e-03 0.00000000e+00\n",
            "  0.00000000e+00 5.15714286e-02 3.61285714e-01 0.00000000e+00\n",
            "  1.52000000e-01 2.71428571e-03]\n",
            " [6.57142857e-03 0.00000000e+00 0.00000000e+00 7.05857143e-01\n",
            "  1.97428571e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 9.01428571e-02]\n",
            " [2.85714286e-04 4.28571429e-04 4.28571429e-04 1.00000000e-03\n",
            "  0.00000000e+00 3.61428571e-02 1.75000000e-01 4.31428571e-01\n",
            "  0.00000000e+00 3.55285714e-01]\n",
            " [4.34428571e-01 0.00000000e+00 0.00000000e+00 1.02857143e-02\n",
            "  5.18000000e-01 0.00000000e+00 2.00000000e-03 0.00000000e+00\n",
            "  0.00000000e+00 3.52857143e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}